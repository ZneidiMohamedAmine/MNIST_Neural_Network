{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# If True, print the name of the GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import struct\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  \n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += 1.0 * out.grad\n",
    "      other.grad += 1.0 * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float))\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "    return self * other**-1\n",
    "\n",
    "  def __neg__(self): # -self\n",
    "    return self * -1\n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "    return self + other\n",
    "\n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "    out = Value(t, (self, ), 'tanh')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += (1 - t**2) * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self, ), 'exp')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += out.data * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  \n",
    "  def backward(self):\n",
    "    \n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self)\n",
    "    \n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# Neuron equivalent in PyTorch\n",
    "class Neuron(nn.Module):\n",
    "    def __init__(self, nin):\n",
    "        super(Neuron, self).__init__()\n",
    "        self.fc = nn.Linear(nin, 1)  # Fully connected layer for a single output\n",
    "  \n",
    "    def forward(self, x):\n",
    "        act = self.fc(x)  # Linear transformation\n",
    "        out = torch.tanh(act)  # Apply Tanh activation function\n",
    "        return out\n",
    "\n",
    "# Layer equivalent in PyTorch\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super(Layer, self).__init__()\n",
    "        self.neurons = nn.ModuleList([Neuron(nin) for _ in range(nout)])  # Create `nout` neurons\n",
    "  \n",
    "    def forward(self, x):\n",
    "        outs = [neuron(x) for neuron in self.neurons]  # Forward pass through each neuron\n",
    "        return outs[0] if len(outs) == 1 else torch.cat(outs, dim=-1)  # Return single output or concatenate outputs\n",
    "\n",
    "# MLP (Multi-Layer Perceptron) equivalent in PyTorch\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        super(MLP, self).__init__()\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = nn.ModuleList([Layer(sz[i], sz[i+1]) for i in range(len(nouts))])  # Create layers\n",
    "  \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # Pass input through each layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n = MLP(784, [523, 10])\n",
    "n =  n.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 images with size 28x28\n",
      "Loaded 1000 labels\n"
     ]
    }
   ],
   "source": [
    "def read_idx3_ubyte(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the magic number\n",
    "        magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        \n",
    "        # Read the image data\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "        \n",
    "    return images\n",
    "\n",
    "def read_idx1_ubyte(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the magic number and number of items\n",
    "        magic, num_items = struct.unpack(\">II\", f.read(8))\n",
    "        \n",
    "        # Read the label data\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        \n",
    "    return labels\n",
    "\n",
    "# Function to convert labels to one-hot encoding\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    one_hot_labels = np.zeros((len(labels), num_classes))\n",
    "    one_hot_labels[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "# Example usage:\n",
    "image_file_path = 'C:\\\\Users\\\\Shadow\\\\Downloads\\\\MNIST_ORG\\\\t10k-images.idx3-ubyte'\n",
    "label_file_path = 'C:\\\\Users\\\\Shadow\\\\Downloads\\\\MNIST_ORG\\\\t10k-labels.idx1-ubyte'\n",
    "\n",
    "# Load images and labels\n",
    "images = read_idx3_ubyte(image_file_path)\n",
    "labels = read_idx1_ubyte(label_file_path)\n",
    "\n",
    "# Limit to the first 1000 images and labels\n",
    "limit = 1000\n",
    "images = images[:limit]  # Get only the first 1000 images\n",
    "labels = labels[:limit]  # Get only the first 1000 labels\n",
    "\n",
    "print(f\"Loaded {len(images)} images with size {images.shape[1]}x{images.shape[2]}\")\n",
    "print(f\"Loaded {len(labels)} labels\")\n",
    "\n",
    "# Flatten the images (28x28 -> 784) and normalize\n",
    "xs = [img.flatten() / 255.0 for img in images]\n",
    "ys = one_hot_encode(labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(values):\n",
    "    # Compute exponentials of all values\n",
    "    exp_vals = [v.exp() for v in values]\n",
    "    \n",
    "    # Sum of exponentials\n",
    "    sum_exp_vals = sum(exp_vals)\n",
    "    \n",
    "    # Normalize each value\n",
    "    softmax_vals = [v / sum_exp_vals for v in exp_vals]\n",
    "    return softmax_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Operation\n",
      "Iteration 0, Loss: 0.9019241333007812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert xs and ys to NumPy arrays before converting to tensors\n",
    "xs_np = np.array(xs)  # Convert list of numpy arrays to a single numpy array\n",
    "ys_np = np.array(ys)\n",
    "\n",
    "# Now convert these NumPy arrays to PyTorch tensors\n",
    "xs_tensor = torch.tensor(xs_np, dtype=torch.float32).to(device)\n",
    "ys_tensor = torch.tensor(ys_np, dtype=torch.float32).to(device)\n",
    "\n",
    "print('Starting Operation')\n",
    "learning_rate = 0.5\n",
    "for k in range(20):\n",
    "    # Forward pass: compute predictions\n",
    "    ypred = [n(x).to(device) for x in xs_tensor]\n",
    "\n",
    "    # Ensure ypred is on the same device as ys_tensor\n",
    "    ypred = [yp.to(device) for yp in ypred]\n",
    "    \n",
    "    # Your softmax and loss operations\n",
    "    ypred_softmax = [softmax(yp) for yp in ypred]\n",
    "\n",
    "    # Loss calculation, ensuring both predicted and true values are on GPU\n",
    "    loss = torch.tensor(0., device=device)\n",
    "    for i in range(len(ys_tensor)):\n",
    "        ytrue = ys_tensor[i]\n",
    "        for j in range(len(ytrue)):\n",
    "            y_pred_prob = ypred_softmax[i][j].to(device)\n",
    "            y_true_val = ytrue[j].to(device)\n",
    "            loss += ((y_pred_prob - y_true_val)**2).to(device)\n",
    "\n",
    "    loss /= len(ys_tensor)\n",
    "    print(f\"Iteration {k}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "   # if k > 7:\n",
    "    #    learning_rate = max(0.1, learning_rate - 0.5)\n",
    "\n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        for p in n.parameters():\n",
    "            p.data.add_(-learning_rate * p.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0542,  0.0652, -0.0181,  0.0588, -0.1418,  0.1642, -0.2404,  0.0088,\n",
      "          0.1769,  0.0257]], device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def image_to_binary(image_path, threshold=128):\n",
    "    \n",
    "    img = Image.open(image_path).convert('L')  \n",
    "    \n",
    "    \n",
    "    img_np = np.array(img)\n",
    "    \n",
    "    \n",
    "    binary_img = (img_np > threshold).astype(np.uint8)\n",
    "    \n",
    "    return binary_img\n",
    "\n",
    "\n",
    "def prepare_tensor_for_model(binary_img):\n",
    "    \n",
    "    binary_tensor = torch.tensor(binary_img, dtype=torch.float32).unsqueeze(0)\n",
    "    return binary_tensor\n",
    "\n",
    "# Example usage\n",
    "image_path = 'C:\\\\Users\\\\Shadow\\\\Documents\\\\nn_number_test\\\\3.png'\n",
    "binary_image = image_to_binary(image_path)\n",
    "binary_tensor = prepare_tensor_for_model(binary_image)\n",
    "\n",
    "\n",
    "binary_tensor = binary_tensor.view(-1, 28*28).to(device)\n",
    "\n",
    "\n",
    "output = n(binary_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
